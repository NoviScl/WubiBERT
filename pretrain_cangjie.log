/mnt/datadisk0/scl/baike/hdf5_cangjie_zh_22675_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5
Logs written to /mnt/datadisk0/scl/results/bert_lamb_pretraining.pyt_bert_pretraining_phase1_fp16_gbs65536.210413140119.log
+ '[' -z /mnt/datadisk0/scl/results/bert_lamb_pretraining.pyt_bert_pretraining_phase1_fp16_gbs65536.210413140119.log ']'
+ tee /mnt/datadisk0/scl/results/bert_lamb_pretraining.pyt_bert_pretraining_phase1_fp16_gbs65536.210413140119.log
+ python3 -m torch.distributed.launch --master_port=1234 --nproc_per_node=8 ./run_pretraining.py --input_dir=/mnt/datadisk0/scl/baike/hdf5_cangjie_zh_22675_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5 --output_dir=/mnt/datadisk0/scl/results/checkpoints_cangjie_22675 --config_file=configs/bert_config_vocab22675.json --bert_model=bert-tiny-uncased --train_batch_size=8192 --max_seq_length=128 --max_predictions_per_seq=20 --max_steps=7038 --warmup_proportion=0.2843 --num_steps_per_checkpoint=800 --learning_rate=10e-3 --seed=12439 --fp16 --gradient_accumulation_steps=32 --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --do_train --do_eval --json-summary /mnt/datadisk0/scl/results/dllogger.json
device: cuda:3 n_gpu: 1, distributed training: True, 16-bits training: True
device: cuda:7 n_gpu: 1, distributed training: True, 16-bits training: True
device: cuda:6 n_gpu: 1, distributed training: True, 16-bits training: True
device: cuda:4 n_gpu: 1, distributed training: True, 16-bits training: True
device: cuda:2 n_gpu: 1, distributed training: True, 16-bits training: True
device: cuda:1 n_gpu: 1, distributed training: True, 16-bits training: True
device: cuda:5 n_gpu: 1, distributed training: True, 16-bits training: True
device: cuda:0 n_gpu: 1, distributed training: True, 16-bits training: True
DLL 2021-04-13 14:01:21.714018 - PARAMETER Config : ["Namespace(allreduce_post_accumulation=True, allreduce_post_accumulation_fp16=True, amp=False, bert_model='bert-tiny-uncased', checkpoint_activations=False, config_file='configs/bert_config_vocab22675.json', disable_progress_bar=False, do_eval=True, do_train=True, fp16=True, gradient_accumulation_steps=32, init_checkpoint=None, init_loss_scale=1048576, input_dir='/mnt/datadisk0/scl/baike/hdf5_cangjie_zh_22675_lower_case_1_seq_len_128_max_pred_20_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5', json_summary='/mnt/datadisk0/scl/results/dllogger.json', learning_rate=0.01, local_rank=0, log_freq=1.0, loss_scale=0.0, max_predictions_per_seq=20, max_seq_length=128, max_steps=7038.0, n_gpu=1, num_steps_per_checkpoint=800, num_train_epochs=3.0, output_dir='/mnt/datadisk0/scl/results/checkpoints_cangjie_22675', phase1_end_step=7038, phase2=False, resume_from_checkpoint=False, resume_step=-1, seed=12439, skip_checkpoint=False, steps_this_run=7038.0, train_batch_size=256, use_env=False, warmup_proportion=0.2843)"] 
